{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.hooks import EarlyStoppingHook\n",
    "from mmengine.visualization import Visualizer, WandbVisBackend\n",
    "from mmdet.utils import setup_cache_size_limit_of_dynamo\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "# mmdetection을 시스템 경로에 추가\n",
    "sys.path.insert(0, \"../mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 맞춤형 학습 설정\n",
    "config_path = './configs/dino/dino-5scale_swin-l_8xb2-12e_coco.py'  # DINO 설정 파일 경로\n",
    "work_dir = './work_dirs/dino_custom'  # 로그와 모델을 저장할 디렉토리 경로\n",
    "train_data_root = '/data/ephemeral/home/dataset/'  # 학습 데이터 경로\n",
    "original_ann_file = '/data/ephemeral/home/dataset/train.json'  # 전체 데이터 어노테이션 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 나누기\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# 이미지 ID 리스트 가져오기\n",
    "image_ids = [image['id'] for image in annotations['images']]\n",
    "\n",
    "# 학습 및 검증 셋으로 분할 (80% 학습, 20% 검증)\n",
    "train_ids, val_ids = train_test_split(image_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# 학습 및 검증 데이터 어노테이션 생성\n",
    "train_annotations = {\n",
    "    'images': [img for img in annotations['images'] if img['id'] in train_ids],\n",
    "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in train_ids],\n",
    "    'categories': annotations['categories']\n",
    "}\n",
    "val_annotations = {\n",
    "    'images': [img for img in annotations['images'] if img['id'] in val_ids],\n",
    "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in val_ids],\n",
    "    'categories': annotations['categories']\n",
    "}\n",
    "\n",
    "# 분할된 어노테이션을 파일로 저장\n",
    "train_ann_file = '/data/ephemeral/home/dataset/train_split.json'\n",
    "val_ann_file = '/data/ephemeral/home/dataset/val_split.json'\n",
    "\n",
    "with open(train_ann_file, 'w') as f:\n",
    "    json.dump(train_annotations, f)\n",
    "with open(val_ann_file, 'w') as f:\n",
    "    json.dump(val_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어노테이션 파일 수정: category_id를 1부터 시작하도록 조정\n",
    "def increment_category_id(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 'categories' 섹션의 'id'를 1부터 시작하도록 수정\n",
    "    id_mapping = {}\n",
    "    for category in data['categories']:\n",
    "        old_id = category['id']\n",
    "        new_id = old_id + 1\n",
    "        id_mapping[old_id] = new_id\n",
    "        category['id'] = new_id\n",
    "\n",
    "    # 'annotations' 섹션의 'category_id'를 매핑에 따라 수정\n",
    "    for ann in data['annotations']:\n",
    "        old_cat_id = ann['category_id']\n",
    "        if old_cat_id in id_mapping:\n",
    "            ann['category_id'] = id_mapping[old_cat_id]\n",
    "        else:\n",
    "            print(f\"Warning: annotation id {ann['id']} has invalid category_id {old_cat_id}\")\n",
    "\n",
    "    # 수정된 데이터를 원래 파일에 저장\n",
    "    with open(annotation_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# 어노테이션 파일에 category_id를 1부터 시작하도록 수정\n",
    "increment_category_id(train_ann_file)\n",
    "increment_category_id(val_ann_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 설정\n",
    "classes = (\n",
    "    \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\",\n",
    "    \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"\n",
    ")\n",
    "num_classes = len(classes)  # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoungtae0818\u001b[0m (\u001b[33myoungtae0818-naver\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/baseline/mmdetection/wandb/run-20241006_234553-0ngyrcab</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/0ngyrcab' target=\"_blank\">dino-5scale_last_layer_20241006_234552</a></strong> to <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/0ngyrcab' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/0ngyrcab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Run Initialized: dino-5scale_last_layer_20241006_234552\n"
     ]
    }
   ],
   "source": [
    "# 설정 파일 로드\n",
    "cfg = Config.fromfile(config_path)\n",
    "\n",
    "# 작업 디렉토리 수정\n",
    "cfg.work_dir = work_dir  # 로그와 모델 저장을 위한 작업 디렉토리 수정\n",
    "\n",
    "# EarlyStoppingHook 추가\n",
    "early_stopping_hook = dict(\n",
    "    type='EarlyStoppingHook',\n",
    "    monitor='bbox_mAP',\n",
    "    rule='greater',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    check_finite=True,\n",
    "    stopping_threshold=None\n",
    ")\n",
    "\n",
    "# cfg.default_hooks에 EarlyStoppingHook 추가\n",
    "cfg.default_hooks.update(\n",
    "    early_stopping=early_stopping_hook\n",
    ")\n",
    "\n",
    "# WandB 초기화 (Runner 전에)\n",
    "run_name = f'dino-5scale_last_layer_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "wandb.init(\n",
    "    project='Object_detection',\n",
    "    name=run_name,\n",
    "    config=cfg.to_dict(),\n",
    "    allow_val_change=True,\n",
    "    reinit=True\n",
    ")\n",
    "print(f\"WandB Run Initialized: {run_name}\")\n",
    "\n",
    "# WandbVisBackend 설정\n",
    "wandb_vis_backend = dict(\n",
    "    type='WandbVisBackend',\n",
    "    save_dir=cfg.work_dir,  # 저장할 디렉토리\n",
    "    init_kwargs=dict(\n",
    "        project='Object_detection',  # WandB 프로젝트 이름\n",
    "        name=run_name,                # 고유한 실행 이름\n",
    "        allow_val_change=True         # 설정 값 변경 허용\n",
    "    ),\n",
    "    define_metric_cfg=None,\n",
    "    commit=True,\n",
    "    log_code_name=None,\n",
    "    watch_kwargs=None\n",
    ")\n",
    "\n",
    "# Visualizer 설정을 Runner의 visualizer 필드로 추가\n",
    "cfg.visualizer = dict(\n",
    "    type='Visualizer',\n",
    "    vis_backends=[\n",
    "        wandb_vis_backend  # WandbVisBackend 추가\n",
    "    ],\n",
    "    name='visualizer'  # Visualizer 이름 (옵션)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 루트 경로 수정\n",
    "cfg.train_dataloader.dataset.ann_file = train_ann_file  # 학습 데이터 어노테이션 파일 경로 수정\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img=train_data_root)  # 학습 데이터 이미지 경로 수정\n",
    "\n",
    "cfg.val_dataloader.dataset.ann_file = val_ann_file  # 검증 데이터 어노테이션 파일 경로 수정\n",
    "cfg.val_dataloader.dataset.data_prefix = dict(img=train_data_root)  # 검증 데이터 이미지 경로 수정\n",
    "\n",
    "# 클래스 설정을 데이터셋의 metainfo에 추가\n",
    "cfg.train_dataloader.dataset.metainfo = dict(classes=classes)  # 학습 데이터셋 클래스 설정\n",
    "cfg.val_dataloader.dataset.metainfo = dict(classes=classes)    # 검증 데이터셋 클래스 설정\n",
    "\n",
    "# 클래스 수 수정\n",
    "cfg.model.bbox_head.num_classes = num_classes  # 모델의 클래스 수 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **모델의 Backbone 동결**\n",
    "# cfg.model.backbone.frozen_stages = 4  # Backbone의 모든 스테이지를 동결\n",
    "\n",
    "# **옵티마이저 설정 조정 (옵션)**\n",
    "# Backbone의 학습률을 설정할 필요가 없으므로 paramwise_cfg를 제거하거나 조정\n",
    "cfg.optim_wrapper.paramwise_cfg = None  # Backbone의 lr_mult 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 혼합 정밀도 학습 사용 설정\n",
    "use_amp = True  # 자동 혼합 정밀도(AMP) 학습 사용 여부\n",
    "if use_amp:\n",
    "    cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "    cfg.optim_wrapper.loss_scale = 'dynamic'\n",
    "\n",
    "# 학습률 자동 스케일링 설정\n",
    "auto_scale_lr = False  # 학습률 자동 스케일링 사용 여부\n",
    "if auto_scale_lr:\n",
    "    if 'auto_scale_lr' in cfg and 'enable' in cfg.auto_scale_lr and 'base_batch_size' in cfg.auto_scale_lr:\n",
    "        cfg.auto_scale_lr.enable = True\n",
    "    else:\n",
    "        raise RuntimeError('설정 파일에 \"auto_scale_lr\" 또는 필요한 키가 없습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정 최적화\n",
    "cfg.train_dataloader.batch_size = 1  # 배치 사이즈를 1로 줄임\n",
    "cfg.val_dataloader.batch_size = 1\n",
    "\n",
    "cfg.train_dataloader.num_workers = 2  # 워커 수 줄이기\n",
    "cfg.val_dataloader.num_workers = 2\n",
    "\n",
    "# Prefetch factor와 persistent_workers 설정\n",
    "cfg.train_dataloader.prefetch_factor = 2\n",
    "cfg.train_dataloader.persistent_workers = False\n",
    "cfg.val_dataloader.prefetch_factor = 2\n",
    "cfg.val_dataloader.persistent_workers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 재개 설정\n",
    "resume_training = False  # 학습 재개 여부 설정 (True로 설정 시 학습 재개)\n",
    "resume_checkpoint_path = None  # 학습 재개 시 체크포인트 경로 지정\n",
    "if resume_training:\n",
    "    if resume_checkpoint_path is None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    else:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = resume_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복적인 컴파일 횟수를 줄여 학습 속도 향상\n",
    "setup_cache_size_limit_of_dynamo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 설정 추가 및 수정\n",
    "cfg.evaluation = dict(interval=1, metric='bbox', save_best='auto')\n",
    "\n",
    "# 평가자(evaluator) 설정 수정\n",
    "if hasattr(cfg, 'val_evaluator'):\n",
    "    # 평가자가 딕셔너리 또는 리스트로 정의되어 있을 경우\n",
    "    if isinstance(cfg.val_evaluator, dict):\n",
    "        cfg.val_evaluator.ann_file = val_ann_file\n",
    "    elif isinstance(cfg.val_evaluator, list):\n",
    "        for evaluator in cfg.val_evaluator:\n",
    "            evaluator['ann_file'] = val_ann_file\n",
    "elif 'evaluation' in cfg:\n",
    "    # 평가 섹션 내에 ann_file을 추가\n",
    "    cfg.evaluation['ann_file'] = val_ann_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모든 레이어를 동결\n",
    "# for name, param in runner.model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # bbox_head만 unfreeze\n",
    "# for param in runner.model.bbox_head.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # 백본의 마지막 두 레이어 unfreeze\n",
    "# for name, param in runner.model.backbone.named_parameters():\n",
    "#     if 'layers.2' in name or 'layers.3' in name:\n",
    "#         param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/06 23:45:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 112301514\n",
      "    GPU 0: Tesla V100-SXM2-32GB\n",
      "    CUDA_HOME: None\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.12.1+cu116\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.6\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.3.2  (built against CUDA 11.5)\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.13.1+cu116\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.10.5\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 112301514\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "10/06 23:45:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16)\n",
      "backend_args = None\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    early_stopping=dict(\n",
      "        check_finite=True,\n",
      "        min_delta=0.001,\n",
      "        monitor='bbox_mAP',\n",
      "        patience=5,\n",
      "        rule='greater',\n",
      "        stopping_threshold=None,\n",
      "        type='EarlyStoppingHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "evaluation = dict(interval=1, metric='bbox', save_best='auto')\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "max_epochs = 100\n",
      "model = dict(\n",
      "    as_two_stage=True,\n",
      "    backbone=dict(\n",
      "        attn_drop_rate=0.0,\n",
      "        convert_weights=True,\n",
      "        depths=[\n",
      "            2,\n",
      "            2,\n",
      "            18,\n",
      "            2,\n",
      "        ],\n",
      "        drop_path_rate=0.2,\n",
      "        drop_rate=0.0,\n",
      "        embed_dims=192,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',\n",
      "            type='Pretrained'),\n",
      "        mlp_ratio=4,\n",
      "        num_heads=[\n",
      "            6,\n",
      "            12,\n",
      "            24,\n",
      "            48,\n",
      "        ],\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        patch_norm=True,\n",
      "        pretrain_img_size=384,\n",
      "        qk_scale=None,\n",
      "        qkv_bias=True,\n",
      "        type='SwinTransformer',\n",
      "        window_size=12,\n",
      "        with_cp=True),\n",
      "    bbox_head=dict(\n",
      "        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            alpha=0.25,\n",
      "            gamma=2.0,\n",
      "            loss_weight=1.0,\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_iou=dict(loss_weight=2.0, type='GIoULoss'),\n",
      "        num_classes=10,\n",
      "        sync_cls_avg_factor=True,\n",
      "        type='DINOHead'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=1,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    decoder=dict(\n",
      "        layer_cfg=dict(\n",
      "            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=5),\n",
      "            ffn_cfg=dict(\n",
      "                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),\n",
      "            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),\n",
      "        num_layers=6,\n",
      "        post_norm_cfg=None,\n",
      "        return_intermediate=True),\n",
      "    dn_cfg=dict(\n",
      "        box_noise_scale=1.0,\n",
      "        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),\n",
      "        label_noise_scale=0.5),\n",
      "    encoder=dict(\n",
      "        layer_cfg=dict(\n",
      "            ffn_cfg=dict(\n",
      "                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),\n",
      "            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=5)),\n",
      "        num_layers=6),\n",
      "    neck=dict(\n",
      "        act_cfg=None,\n",
      "        in_channels=[\n",
      "            192,\n",
      "            384,\n",
      "            768,\n",
      "            1536,\n",
      "        ],\n",
      "        kernel_size=1,\n",
      "        norm_cfg=dict(num_groups=32, type='GN'),\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='ChannelMapper'),\n",
      "    num_feature_levels=5,\n",
      "    num_queries=900,\n",
      "    positional_encoding=dict(\n",
      "        normalize=True, num_feats=128, offset=0.0, temperature=20),\n",
      "    test_cfg=dict(max_per_img=300),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            match_costs=[\n",
      "                dict(type='FocalLossCost', weight=2.0),\n",
      "                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),\n",
      "                dict(iou_mode='giou', type='IoUCost', weight=2.0),\n",
      "            ],\n",
      "            type='HungarianAssigner')),\n",
      "    type='DINO',\n",
      "    with_box_refine=True)\n",
      "num_levels = 5\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=dict(max_norm=0.1, norm_type=2),\n",
      "    loss_scale='dynamic',\n",
      "    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.0001),\n",
      "    paramwise_cfg=None,\n",
      "    type='AmpOptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=100,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=100, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='/data/ephemeral/home/dataset/train_split.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='/data/ephemeral/home/dataset/'),\n",
      "        data_root='data/coco/',\n",
      "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'General trash',\n",
      "                'Paper',\n",
      "                'Paper pack',\n",
      "                'Metal',\n",
      "                'Glass',\n",
      "                'Plastic',\n",
      "                'Styrofoam',\n",
      "                'Plastic bag',\n",
      "                'Battery',\n",
      "                'Clothing',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(\n",
      "                transforms=[\n",
      "                    [\n",
      "                        dict(\n",
      "                            keep_ratio=True,\n",
      "                            scales=[\n",
      "                                (\n",
      "                                    480,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    512,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    544,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    576,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    608,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    640,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    672,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    704,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    736,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    768,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    800,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                            ],\n",
      "                            type='RandomChoiceResize'),\n",
      "                    ],\n",
      "                    [\n",
      "                        dict(\n",
      "                            keep_ratio=True,\n",
      "                            scales=[\n",
      "                                (\n",
      "                                    400,\n",
      "                                    4200,\n",
      "                                ),\n",
      "                                (\n",
      "                                    500,\n",
      "                                    4200,\n",
      "                                ),\n",
      "                                (\n",
      "                                    600,\n",
      "                                    4200,\n",
      "                                ),\n",
      "                            ],\n",
      "                            type='RandomChoiceResize'),\n",
      "                        dict(\n",
      "                            allow_negative_crop=True,\n",
      "                            crop_size=(\n",
      "                                384,\n",
      "                                600,\n",
      "                            ),\n",
      "                            crop_type='absolute_range',\n",
      "                            type='RandomCrop'),\n",
      "                        dict(\n",
      "                            keep_ratio=True,\n",
      "                            scales=[\n",
      "                                (\n",
      "                                    480,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    512,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    544,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    576,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    608,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    640,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    672,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    704,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    736,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    768,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                                (\n",
      "                                    800,\n",
      "                                    1333,\n",
      "                                ),\n",
      "                            ],\n",
      "                            type='RandomChoiceResize'),\n",
      "                    ],\n",
      "                ],\n",
      "                type='RandomChoice'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=False,\n",
      "    prefetch_factor=2,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(\n",
      "                    keep_ratio=True,\n",
      "                    scales=[\n",
      "                        (\n",
      "                            480,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            512,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            544,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            576,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            608,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            640,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            672,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            704,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            736,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            768,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            800,\n",
      "                            1333,\n",
      "                        ),\n",
      "                    ],\n",
      "                    type='RandomChoiceResize'),\n",
      "            ],\n",
      "            [\n",
      "                dict(\n",
      "                    keep_ratio=True,\n",
      "                    scales=[\n",
      "                        (\n",
      "                            400,\n",
      "                            4200,\n",
      "                        ),\n",
      "                        (\n",
      "                            500,\n",
      "                            4200,\n",
      "                        ),\n",
      "                        (\n",
      "                            600,\n",
      "                            4200,\n",
      "                        ),\n",
      "                    ],\n",
      "                    type='RandomChoiceResize'),\n",
      "                dict(\n",
      "                    allow_negative_crop=True,\n",
      "                    crop_size=(\n",
      "                        384,\n",
      "                        600,\n",
      "                    ),\n",
      "                    crop_type='absolute_range',\n",
      "                    type='RandomCrop'),\n",
      "                dict(\n",
      "                    keep_ratio=True,\n",
      "                    scales=[\n",
      "                        (\n",
      "                            480,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            512,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            544,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            576,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            608,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            640,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            672,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            704,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            736,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            768,\n",
      "                            1333,\n",
      "                        ),\n",
      "                        (\n",
      "                            800,\n",
      "                            1333,\n",
      "                        ),\n",
      "                    ],\n",
      "                    type='RandomChoiceResize'),\n",
      "            ],\n",
      "        ],\n",
      "        type='RandomChoice'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='/data/ephemeral/home/dataset/val_split.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='/data/ephemeral/home/dataset/'),\n",
      "        data_root='data/coco/',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'General trash',\n",
      "                'Paper',\n",
      "                'Paper pack',\n",
      "                'Metal',\n",
      "                'Glass',\n",
      "                'Plastic',\n",
      "                'Styrofoam',\n",
      "                'Plastic bag',\n",
      "                'Battery',\n",
      "                'Clothing',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=False,\n",
      "    prefetch_factor=2,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='/data/ephemeral/home/dataset/val_split.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='Visualizer',\n",
      "    vis_backends=[\n",
      "        dict(\n",
      "            commit=True,\n",
      "            define_metric_cfg=None,\n",
      "            init_kwargs=dict(\n",
      "                allow_val_change=True,\n",
      "                name='dino-5scale_last_layer_20241006_234552',\n",
      "                project='Object_detection'),\n",
      "            log_code_name=None,\n",
      "            save_dir='./work_dirs/dino_custom',\n",
      "            type='WandbVisBackend',\n",
      "            watch_kwargs=None),\n",
      "    ])\n",
      "work_dir = './work_dirs/dino_custom'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0ngyrcab) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dino-5scale_last_layer_20241006_234552</strong> at: <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/0ngyrcab' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/0ngyrcab</a><br/> View project at: <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241006_234553-0ngyrcab/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0ngyrcab). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./work_dirs/dino_custom/wandb/run-20241006_234556-tgzkukzh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/tgzkukzh' target=\"_blank\">dino-5scale_last_layer_20241006_234552</a></strong> to <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/tgzkukzh' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/tgzkukzh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/06 23:46:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "10/06 23:46:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "10/06 23:46:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\n",
      "10/06 23:46:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/06 23:46:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "10/06 23:46:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /data/ephemeral/home/baseline/mmdetection/work_dirs/dino_custom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/baseline/mmdetection/mmdet/models/layers/positional_encoding.py:103: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature**(2 * (dim_t // 2) / self.num_feats)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/ephemeral/home/baseline/mmdetection/mmdet/models/layers/transformer/utils.py:71: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature**(2 * (dim_t // 2) / num_feats)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation annotation file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_ann_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/runner.py:1777\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_compile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1777\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/loops.py:98\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mval_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_begin\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m                  \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/loops.py:115\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/loops.py:131\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[0;34m(self, idx, data_batch)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_idx\u001b[38;5;241m=\u001b[39midx, data_batch\u001b[38;5;241m=\u001b[39mdata_batch)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    136\u001b[0m     batch_idx\u001b[38;5;241m=\u001b[39midx,\n\u001b[1;32m    137\u001b[0m     data_batch\u001b[38;5;241m=\u001b[39mdata_batch,\n\u001b[1;32m    138\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:114\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m optim_wrapper\u001b[38;5;241m.\u001b[39moptim_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    113\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_preprocessor(data, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 114\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    115\u001b[0m parsed_losses, log_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_losses(losses)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    116\u001b[0m optim_wrapper\u001b[38;5;241m.\u001b[39mupdate_params(parsed_losses)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:361\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    363\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/detectors/base.py:92\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The unified entry for a forward process in both training and test.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mThe method should accept three modes: \"tensor\", \"predict\" and \"loss\":\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    - If ``mode=\"loss\"``, return a dict of tensor.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(inputs, data_samples)\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/detectors/base_detr.py:101\u001b[0m, in \u001b[0;36mDetectionTransformer.loss\u001b[0;34m(self, batch_inputs, batch_data_samples)\u001b[0m\n\u001b[1;32m     98\u001b[0m img_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_feat(batch_inputs)\n\u001b[1;32m     99\u001b[0m head_inputs_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_transformer(img_feats,\n\u001b[1;32m    100\u001b[0m                                             batch_data_samples)\n\u001b[0;32m--> 101\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbbox_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhead_inputs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/dense_heads/dino_head.py:74\u001b[0m, in \u001b[0;36mDINOHead.loss\u001b[0;34m(self, hidden_states, references, enc_outputs_class, enc_outputs_coord, batch_data_samples, dn_meta)\u001b[0m\n\u001b[1;32m     71\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(hidden_states, references)\n\u001b[1;32m     72\u001b[0m loss_inputs \u001b[38;5;241m=\u001b[39m outs \u001b[38;5;241m+\u001b[39m (enc_outputs_class, enc_outputs_coord,\n\u001b[1;32m     73\u001b[0m                       batch_gt_instances, batch_img_metas, dn_meta)\n\u001b[0;32m---> 74\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_by_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/dense_heads/dino_head.py:150\u001b[0m, in \u001b[0;36mDINOHead.loss_by_feat\u001b[0;34m(self, all_layers_cls_scores, all_layers_bbox_preds, enc_cls_scores, enc_bbox_preds, batch_gt_instances, batch_img_metas, dn_meta, batch_gt_instances_ignore)\u001b[0m\n\u001b[1;32m    146\u001b[0m     loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menc_loss_iou\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m enc_losses_iou\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_layers_denoising_cls_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# calculate denoising loss from all decoder layers\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     dn_losses_cls, dn_losses_bbox, dn_losses_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_dn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_layers_denoising_cls_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_layers_denoising_bbox_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_gt_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_gt_instances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdn_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdn_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# collate denoising loss\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdn_loss_cls\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dn_losses_cls[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/dense_heads/dino_head.py:197\u001b[0m, in \u001b[0;36mDINOHead.loss_dn\u001b[0;34m(self, all_layers_denoising_cls_scores, all_layers_denoising_bbox_preds, batch_gt_instances, batch_img_metas, dn_meta)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_dn\u001b[39m(\u001b[38;5;28mself\u001b[39m, all_layers_denoising_cls_scores: Tensor,\n\u001b[1;32m    169\u001b[0m             all_layers_denoising_bbox_preds: Tensor,\n\u001b[1;32m    170\u001b[0m             batch_gt_instances: InstanceList, batch_img_metas: List[\u001b[38;5;28mdict\u001b[39m],\n\u001b[1;32m    171\u001b[0m             dn_meta: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Tensor]]:\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate denoising loss.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        of each decoder layers.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmulti_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_dn_single\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_layers_denoising_cls_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_layers_denoising_bbox_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_gt_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_gt_instances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdn_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdn_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/utils/misc.py:219\u001b[0m, in \u001b[0;36mmulti_apply\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m pfunc \u001b[38;5;241m=\u001b[39m partial(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m func\n\u001b[1;32m    218\u001b[0m map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(pfunc, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_results\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/dense_heads/dino_head.py:271\u001b[0m, in \u001b[0;36mDINOHead._loss_dn_single\u001b[0;34m(self, dn_cls_scores, dn_bbox_preds, batch_gt_instances, batch_img_metas, dn_meta)\u001b[0m\n\u001b[1;32m    266\u001b[0m         loss_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_cls(\n\u001b[1;32m    267\u001b[0m             cls_scores, (labels, scores),\n\u001b[1;32m    268\u001b[0m             weight\u001b[38;5;241m=\u001b[39mlabel_weights,\n\u001b[1;32m    269\u001b[0m             avg_factor\u001b[38;5;241m=\u001b[39mcls_avg_factor)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         loss_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcls_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mavg_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_avg_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     loss_cls \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mcls_scores\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mcls_scores\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/losses/focal_loss.py:243\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, pred, target, weight, avg_factor, reduction_override)\u001b[0m\n\u001b[1;32m    240\u001b[0m             target \u001b[38;5;241m=\u001b[39m target[:, :num_classes]\n\u001b[1;32m    241\u001b[0m             calculate_loss_func \u001b[38;5;241m=\u001b[39m py_sigmoid_focal_loss\n\u001b[0;32m--> 243\u001b[0m     loss_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weight \u001b[38;5;241m*\u001b[39m \u001b[43mcalculate_loss_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mavg_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavg_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/baseline/mmdetection/mmdet/models/losses/focal_loss.py:160\u001b[0m, in \u001b[0;36msigmoid_focal_loss\u001b[0;34m(pred, target, weight, gamma, alpha, reduction, avg_factor)\u001b[0m\n\u001b[1;32m    158\u001b[0m             weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(loss\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m loss\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m--> 160\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mweight_reduce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 설정 파일을 통해 러너 생성\n",
    "runner = Runner.from_cfg(cfg)\n",
    "\n",
    "# 학습 시작 전에 검증 어노테이션 파일 존재 여부 확인\n",
    "if not os.path.exists(val_ann_file):\n",
    "    raise FileNotFoundError(f'Validation annotation file not found: {val_ann_file}')\n",
    "\n",
    "# 학습 시작\n",
    "runner.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

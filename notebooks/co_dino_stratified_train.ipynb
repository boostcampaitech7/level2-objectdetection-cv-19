{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.hooks import EarlyStoppingHook\n",
    "from mmengine.visualization import Visualizer, WandbVisBackend\n",
    "from mmdet.utils import setup_cache_size_limit_of_dynamo\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "# mmdetection을 시스템 경로에 추가\n",
    "sys.path.insert(0, \"../mmdetection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 맞춤형 학습 설정\n",
    "config_path = './projects/CO-DETR/configs/codino/co_dino_5scale_swin_l_lsj_16xb1_1x_coco.py'  # DINO 설정 파일 경로\n",
    "work_dir = './work_dirs/co_dino_custom'  # 로그와 모델을 저장할 디렉토리 경로\n",
    "train_data_root = '/data/ephemeral/home/dataset/'  # 학습 데이터 경로\n",
    "original_ann_file = '/data/ephemeral/home/dataset/train.json'  # 전체 데이터 어노테이션 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 설정\n",
    "classes = (\n",
    "    \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\",\n",
    "    \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"\n",
    ")\n",
    "num_classes = len(classes)  # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 나누기 (멀티라벨 스트라티파이드 분할)\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# 이미지 ID 리스트 가져오기\n",
    "image_ids = [image['id'] for image in annotations['images']]\n",
    "\n",
    "# 이미지 ID와 해당 이미지에 포함된 클래스 목록 매핑\n",
    "image_id_to_classes = {image_id: [] for image_id in image_ids}\n",
    "for ann in annotations['annotations']:\n",
    "    image_id = ann['image_id']\n",
    "    category_id = ann['category_id']\n",
    "    if category_id not in image_id_to_classes[image_id]:\n",
    "        image_id_to_classes[image_id].append(category_id)\n",
    "\n",
    "# 멀티라벨 인코딩을 위한 이진 매트릭스 생성\n",
    "num_images = len(image_ids)\n",
    "label_matrix = np.zeros((num_images, num_classes))\n",
    "image_id_to_index = {image_id: idx for idx, image_id in enumerate(image_ids)}\n",
    "for image_id, class_ids in image_id_to_classes.items():\n",
    "    idx = image_id_to_index[image_id]\n",
    "    for class_id in class_ids:\n",
    "        label_matrix[idx, class_id - 1] = 1  # category_id는 1부터 시작\n",
    "\n",
    "# 멀티라벨 스트라티파이드 셔플 스플릿 사용\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_indices, val_indices = next(msss.split(image_ids, label_matrix))\n",
    "\n",
    "# 인덱스를 이미지 ID로 변환\n",
    "train_ids = [image_ids[idx] for idx in train_indices]\n",
    "val_ids = [image_ids[idx] for idx in val_indices]\n",
    "\n",
    "# 학습 및 검증 데이터 어노테이션 생성\n",
    "train_annotations = {\n",
    "    'images': [img for img in annotations['images'] if img['id'] in train_ids],\n",
    "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in train_ids],\n",
    "    'categories': annotations['categories']\n",
    "}\n",
    "val_annotations = {\n",
    "    'images': [img for img in annotations['images'] if img['id'] in val_ids],\n",
    "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in val_ids],\n",
    "    'categories': annotations['categories']\n",
    "}\n",
    "\n",
    "# 분할된 어노테이션을 파일로 저장\n",
    "train_ann_file = '/data/ephemeral/home/dataset/train_split.json'\n",
    "val_ann_file = '/data/ephemeral/home/dataset/val_split.json'\n",
    "\n",
    "with open(train_ann_file, 'w') as f:\n",
    "    json.dump(train_annotations, f)\n",
    "with open(val_ann_file, 'w') as f:\n",
    "    json.dump(val_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어노테이션 파일 수정: category_id를 1부터 시작하도록 조정\n",
    "def increment_category_id(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 'categories' 섹션의 'id'를 1부터 시작하도록 수정\n",
    "    id_mapping = {}\n",
    "    for category in data['categories']:\n",
    "        old_id = category['id']\n",
    "        new_id = old_id + 1\n",
    "        id_mapping[old_id] = new_id\n",
    "        category['id'] = new_id\n",
    "\n",
    "    # 'annotations' 섹션의 'category_id'를 매핑에 따라 수정\n",
    "    for ann in data['annotations']:\n",
    "        old_cat_id = ann['category_id']\n",
    "        if old_cat_id in id_mapping:\n",
    "            ann['category_id'] = id_mapping[old_cat_id]\n",
    "        else:\n",
    "            print(f\"Warning: annotation id {ann['id']} has invalid category_id {old_cat_id}\")\n",
    "\n",
    "    # 수정된 데이터를 원래 파일에 저장\n",
    "    with open(annotation_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# 어노테이션 파일에 category_id를 1부터 시작하도록 수정\n",
    "increment_category_id(train_ann_file)\n",
    "increment_category_id(val_ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/baseline/mmdetection/wandb/run-20241024_152541-wxkitv4e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/wxkitv4e' target=\"_blank\">co_dino_5scale_swin_l_lsj_20241024_152536</a></strong> to <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/wxkitv4e' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/wxkitv4e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Run Initialized: co_dino_5scale_swin_l_lsj_20241024_152536\n"
     ]
    }
   ],
   "source": [
    "# 설정 파일 로드\n",
    "cfg = Config.fromfile(config_path)\n",
    "\n",
    "# 데이터 전처리기 수정: pad_mask와 pad_seg 비활성화 및 batch_augments 제거\n",
    "cfg.model.data_preprocessor = dict(\n",
    "    type='DetDataPreprocessor',\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375],\n",
    "    bgr_to_rgb=True,\n",
    "    pad_size_divisor=32,\n",
    "    pad_mask=False,       # 마스크 패딩 비활성화\n",
    "    pad_seg=False,        # 세그멘테이션 패딩 비활성화\n",
    "    batch_augments=None   # 배치 증강 비활성화\n",
    ")\n",
    "    \n",
    "\n",
    "# 작업 디렉토리 수정\n",
    "cfg.work_dir = work_dir  # 로그와 모델 저장을 위한 작업 디렉토리 수정\n",
    "\n",
    "# EarlyStoppingHook 추가\n",
    "early_stopping_hook = dict(\n",
    "    type='EarlyStoppingHook',\n",
    "    monitor='bbox_mAP',\n",
    "    rule='greater',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    check_finite=True,\n",
    "    stopping_threshold=None\n",
    ")\n",
    "\n",
    "# cfg.default_hooks에 EarlyStoppingHook 추가\n",
    "cfg.default_hooks.update(\n",
    "    early_stopping=early_stopping_hook\n",
    ")\n",
    "\n",
    "# WandB 초기화 (Runner 전에)\n",
    "run_name = f'co_dino_5scale_swin_l_lsj_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "wandb.init(\n",
    "    project='Object_detection',\n",
    "    name=run_name,\n",
    "    config=cfg.to_dict(),\n",
    "    allow_val_change=True,\n",
    "    reinit=True\n",
    ")\n",
    "print(f\"WandB Run Initialized: {run_name}\")\n",
    "\n",
    "# WandbVisBackend 설정\n",
    "wandb_vis_backend = dict(\n",
    "    type='WandbVisBackend',\n",
    "    save_dir=cfg.work_dir,  # 저장할 디렉토리\n",
    "    init_kwargs=dict(\n",
    "        project='Object_detection',  # WandB 프로젝트 이름\n",
    "        name=run_name,                # 고유한 실행 이름\n",
    "        allow_val_change=True         # 설정 값 변경 허용\n",
    "    ),\n",
    "    define_metric_cfg=None,\n",
    "    commit=True,\n",
    "    log_code_name=None,\n",
    "    watch_kwargs=None\n",
    ")\n",
    "\n",
    "# Visualizer 설정을 Runner의 visualizer 필드로 추가\n",
    "cfg.visualizer = dict(\n",
    "    type='Visualizer',\n",
    "    vis_backends=[\n",
    "        wandb_vis_backend  # WandbVisBackend 추가\n",
    "    ],\n",
    "    name='visualizer'  # Visualizer 이름 (옵션)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 이미지 크기 설정\n",
    "image_size = (1280, 1280)\n",
    "\n",
    "cfg.train_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,  # 디버깅을 위해 워커 수를 0으로 설정\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    dataset=dict(\n",
    "        type='MultiImageMixDataset',  # MultiImageMixDataset 사용\n",
    "        dataset=dict(\n",
    "            type='CocoDataset',  # 기본적으로 CocoDataset 사용\n",
    "            data_root=train_data_root,\n",
    "            ann_file=train_ann_file,\n",
    "            data_prefix=dict(img=train_data_root),\n",
    "            filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
    "            pipeline=[\n",
    "                dict(type='LoadImageFromFile', backend_args=None),\n",
    "                dict(type='LoadAnnotations', with_bbox=True, with_label=True, with_mask=False),  # with_label=True로 수정\n",
    "                dict(\n",
    "                    type='RandomResize',\n",
    "                    scale=image_size,\n",
    "                    ratio_range=(0.1, 2.0),  # LSJ를 위한 비율 범위 수정\n",
    "                    keep_ratio=True\n",
    "                ),\n",
    "                dict(\n",
    "                    type='RandomCrop',\n",
    "                    crop_type='absolute_range',\n",
    "                    crop_size=image_size,\n",
    "                    recompute_bbox=True,\n",
    "                    allow_negative_crop=True\n",
    "                ),\n",
    "                dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),\n",
    "                dict(type='RandomFlip', prob=0.5),\n",
    "                dict(type='Pad', size=image_size),\n",
    "                # dict(type='PackDetInputs')  # 제거\n",
    "            ],\n",
    "            metainfo=dict(classes=classes)\n",
    "        ),\n",
    "        pipeline=[\n",
    "            dict(\n",
    "                type='MixUp',\n",
    "                img_scale=image_size,        # 이미지 크기 설정\n",
    "                ratio_range=(0.1, 2.0),      # 비율 범위 수정\n",
    "                flip_ratio=0.5,\n",
    "                pad_val=114.0,\n",
    "                max_iters=15,\n",
    "                bbox_clip_border=True\n",
    "            ),\n",
    "            dict(type='PackDetInputs')  # PackDetInputs를 MixUp 이후에 추가\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "cfg.val_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,  # 디버깅을 위해 워커 수를 0으로 설정\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type='CocoDataset',  # 검증 데이터는 MultiImageMixDataset 필요 없음\n",
    "        data_root=train_data_root,\n",
    "        ann_file=val_ann_file,\n",
    "        data_prefix=dict(img=train_data_root),\n",
    "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile', backend_args=None),\n",
    "            dict(type='LoadAnnotations', with_bbox=True, with_label=True, with_mask=False),  # with_label=True로 수정\n",
    "            dict(\n",
    "                type='Resize',\n",
    "                scale=image_size,\n",
    "                keep_ratio=True\n",
    "            ),\n",
    "            dict(type='Pad', size=image_size),\n",
    "            dict(type='PackDetInputs')  # PackDetInputs는 검증 데이터에서는 필요\n",
    "        ],\n",
    "        metainfo=dict(classes=classes)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 데이터 파이프라인 수정: 마스크 관련 부분 제거 및 불필요한 단계 제거\n",
    "def modify_pipeline(pipeline):\n",
    "    new_pipeline = []\n",
    "    for step in pipeline:\n",
    "        if step['type'] == 'LoadAnnotations':\n",
    "            step['with_bbox'] = True\n",
    "            step['with_label'] = True  # with_label=True로 수정\n",
    "            step['with_mask'] = False  # 마스크 관련 부분 제거\n",
    "        elif step['type'] == 'FilterAnnotations':\n",
    "            # FilterAnnotations에서 마스크 관련 설정 제거\n",
    "            keys_to_remove = ['min_gt_mask_area', 'min_mask_area', 'mask_thr_binary']\n",
    "            for key in keys_to_remove:\n",
    "                if key in step:\n",
    "                    step.pop(key)\n",
    "        new_pipeline.append(step)\n",
    "    return new_pipeline\n",
    "\n",
    "# 파이프라인 수정 적용\n",
    "cfg.train_dataloader.dataset.dataset.pipeline = modify_pipeline(cfg.train_dataloader.dataset.dataset.pipeline)\n",
    "# cfg.val_dataloader.dataset.pipeline = modify_pipeline(cfg.val_dataloader.dataset.pipeline)\n",
    "\n",
    "\n",
    "# 모델 설정에서 mask_head 제거 또는 비활성화\n",
    "if hasattr(cfg.model, 'mask_head'):\n",
    "    cfg.model.mask_head = None\n",
    "\n",
    "\n",
    "# 클래스 수 수정\n",
    "# bbox_head가 리스트인 경우 모든 헤드에 대해 num_classes 설정\n",
    "if isinstance(cfg.model.bbox_head, list):\n",
    "    for head in cfg.model.bbox_head:\n",
    "        head['num_classes'] = num_classes\n",
    "else:\n",
    "    cfg.model.bbox_head['num_classes'] = num_classes\n",
    "\n",
    "# roi_head 내의 bbox_head도 설정\n",
    "if isinstance(cfg.model.roi_head, list):\n",
    "    for roi_head in cfg.model.roi_head:\n",
    "        if 'bbox_head' in roi_head:\n",
    "            roi_head['bbox_head']['num_classes'] = num_classes\n",
    "else:\n",
    "    if 'bbox_head' in cfg.model.roi_head:\n",
    "        cfg.model.roi_head['bbox_head']['num_classes'] = num_classes\n",
    "\n",
    "# query_head도 num_classes 설정\n",
    "if isinstance(cfg.model.query_head, list):\n",
    "    for q_head in cfg.model.query_head:\n",
    "        q_head['num_classes'] = num_classes\n",
    "else:\n",
    "    cfg.model.query_head['num_classes'] = num_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 관련 헤드에 대해 num_classes가 올바르게 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 추가: 모델의 모든 관련 헤드에 num_classes가 올바르게 설정되었는지 확인\n",
    "def verify_num_classes(cfg, num_classes):\n",
    "    # bbox_head\n",
    "    if isinstance(cfg.model.bbox_head, list):\n",
    "        for head in cfg.model.bbox_head:\n",
    "            assert head.get('num_classes') == num_classes, \"bbox_head num_classes 설정 오류\"\n",
    "    else:\n",
    "        assert cfg.model.bbox_head.get('num_classes') == num_classes, \"bbox_head num_classes 설정 오류\"\n",
    "    \n",
    "    # roi_head.bbox_head\n",
    "    if isinstance(cfg.model.roi_head, list):\n",
    "        for roi_head in cfg.model.roi_head:\n",
    "            if 'bbox_head' in roi_head:\n",
    "                assert roi_head['bbox_head'].get('num_classes') == num_classes, \"roi_head.bbox_head num_classes 설정 오류\"\n",
    "    else:\n",
    "        if 'bbox_head' in cfg.model.roi_head:\n",
    "            assert cfg.model.roi_head['bbox_head'].get('num_classes') == num_classes, \"roi_head.bbox_head num_classes 설정 오류\"\n",
    "    \n",
    "    # query_head\n",
    "    if isinstance(cfg.model.query_head, list):\n",
    "        for q_head in cfg.model.query_head:\n",
    "            assert q_head.get('num_classes') == num_classes, \"query_head num_classes 설정 오류\"\n",
    "    else:\n",
    "        assert cfg.model.query_head.get('num_classes') == num_classes, \"query_head num_classes 설정 오류\"\n",
    "    \n",
    "    print(\"모든 관련 헤드에 대해 num_classes가 올바르게 설정되었습니다.\")\n",
    "\n",
    "verify_num_classes(cfg, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 혼합 정밀도 학습 사용 설정\n",
    "use_amp = True  # AMP 활성화\n",
    "if use_amp:\n",
    "    cfg.optim_wrapper = dict(\n",
    "        type='AmpOptimWrapper',          # AMP 옵티마이저 래퍼 사용\n",
    "        optimizer=cfg.optim_wrapper.optimizer,  # 기존 옵티마이저 설정 유지\n",
    "        accumulative_counts=1            # 그래디언트 누적 단계\n",
    "        # scaler 제거: AMP는 내부적으로 자동 처리됩니다.\n",
    "    )\n",
    "\n",
    "\n",
    "# 학습률 자동 스케일링 설정\n",
    "auto_scale_lr = False  # 학습률 자동 스케일링 사용 여부\n",
    "if auto_scale_lr:\n",
    "    if 'auto_scale_lr' in cfg and 'enable' in cfg.auto_scale_lr and 'base_batch_size' in cfg.auto_scale_lr:\n",
    "        cfg.auto_scale_lr.enable = True\n",
    "    else:\n",
    "        raise RuntimeError('설정 파일에 \"auto_scale_lr\" 또는 필요한 키가 없습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정 최적화\n",
    "cfg.train_dataloader.batch_size = 1  # 배치 사이즈를 1로 줄임\n",
    "cfg.val_dataloader.batch_size = 1\n",
    "\n",
    "cfg.train_dataloader.num_workers = 2  # 워커 수 줄이기\n",
    "cfg.val_dataloader.num_workers = 2\n",
    "\n",
    "# Prefetch factor와 persistent_workers 설정\n",
    "cfg.train_dataloader.prefetch_factor = 2\n",
    "cfg.train_dataloader.persistent_workers = False\n",
    "cfg.val_dataloader.prefetch_factor = 2\n",
    "cfg.val_dataloader.persistent_workers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 재개 설정\n",
    "resume_training = True  # 학습 재개 여부 설정 (True로 설정 시 학습 재개)\n",
    "resume_checkpoint_path = './work_dirs/co_dino_custom/epoch_9.pth'  # 학습 재개 시 체크포인트 경로 지정\n",
    "\n",
    "if resume_training:\n",
    "    if resume_checkpoint_path is None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    else:\n",
    "        cfg.resume = False  # 전체 학습 재개를 비활성화 (옵티마이저 상태 로드를 방지)\n",
    "        cfg.load_from = resume_checkpoint_path  # 모델 가중치만 불러오기\n",
    "        # 옵티마이저의 학습률을 새로 설정\n",
    "        cfg.optim_wrapper.optimizer.lr = 1e-5  # 새로운 학습률 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 설정\n",
    "cfg.optim_wrapper.optimizer.lr = 1e-5  # 학습률을 1e-5로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복적인 컴파일 횟수를 줄여 학습 속도 향상\n",
    "setup_cache_size_limit_of_dynamo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 설정 추가 및 수정\n",
    "cfg.evaluation = dict(interval=1, metric='bbox', save_best='auto')\n",
    "\n",
    "# 평가자(evaluator) 설정 수정\n",
    "if hasattr(cfg, 'val_evaluator'):\n",
    "    # 평가자가 딕셔너리 또는 리스트로 정의되어 있을 경우\n",
    "    if isinstance(cfg.val_evaluator, dict):\n",
    "        cfg.val_evaluator.ann_file = val_ann_file\n",
    "    elif isinstance(cfg.val_evaluator, list):\n",
    "        for evaluator in cfg.val_evaluator:\n",
    "            evaluator['ann_file'] = val_ann_file\n",
    "elif 'evaluation' in cfg:\n",
    "    # 평가 섹션 내에 ann_file을 추가\n",
    "    cfg.evaluation['ann_file'] = val_ann_file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/24 15:25:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1067258404\n",
      "    GPU 0: Tesla V100-SXM2-32GB\n",
      "    CUDA_HOME: None\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.12.1+cu116\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.6\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.3.2  (built against CUDA 11.5)\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.13.1+cu116\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.10.1\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1067258404\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "10/24 15:25:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16)\n",
      "backend_args = None\n",
      "batch_augments = [\n",
      "    dict(pad_mask=True, size=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='BatchFixedSizePad'),\n",
      "]\n",
      "custom_imports = dict(\n",
      "    allow_failed_imports=False, imports=[\n",
      "        'projects.CO-DETR.codetr',\n",
      "    ])\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        _scope_='mmdet',\n",
      "        by_epoch=True,\n",
      "        interval=1,\n",
      "        max_keep_ckpts=3,\n",
      "        type='CheckpointHook'),\n",
      "    early_stopping=dict(\n",
      "        check_finite=True,\n",
      "        min_delta=0.001,\n",
      "        monitor='bbox_mAP',\n",
      "        patience=5,\n",
      "        rule='greater',\n",
      "        stopping_threshold=None,\n",
      "        type='EarlyStoppingHook'),\n",
      "    logger=dict(_scope_='mmdet', interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(_scope_='mmdet', type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(_scope_='mmdet', type='DistSamplerSeedHook'),\n",
      "    timer=dict(_scope_='mmdet', type='IterTimerHook'),\n",
      "    visualization=dict(_scope_='mmdet', type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "evaluation = dict(interval=1, metric='bbox', save_best='auto')\n",
      "image_size = (\n",
      "    1280,\n",
      "    1280,\n",
      ")\n",
      "load_from = './work_dirs/co_dino_custom/epoch_9.pth'\n",
      "load_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(\n",
      "        keep_ratio=True,\n",
      "        ratio_range=(\n",
      "            0.1,\n",
      "            2.0,\n",
      "        ),\n",
      "        scale=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        type='RandomResize'),\n",
      "    dict(\n",
      "        allow_negative_crop=True,\n",
      "        crop_size=(\n",
      "            1280,\n",
      "            1280,\n",
      "        ),\n",
      "        crop_type='absolute_range',\n",
      "        recompute_bbox=True,\n",
      "        type='RandomCrop'),\n",
      "    dict(min_gt_bbox_wh=(\n",
      "        0.01,\n",
      "        0.01,\n",
      "    ), type='FilterAnnotations'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(pad_val=dict(img=(\n",
      "        114,\n",
      "        114,\n",
      "        114,\n",
      "    )), size=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='Pad'),\n",
      "]\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(\n",
      "    _scope_='mmdet', by_epoch=True, type='LogProcessor', window_size=50)\n",
      "loss_lambda = 2.0\n",
      "max_epochs = 50\n",
      "max_iters = 270000\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        attn_drop_rate=0.0,\n",
      "        convert_weights=True,\n",
      "        depths=[\n",
      "            2,\n",
      "            2,\n",
      "            18,\n",
      "            2,\n",
      "        ],\n",
      "        drop_path_rate=0.3,\n",
      "        drop_rate=0.0,\n",
      "        embed_dims=192,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',\n",
      "            type='Pretrained'),\n",
      "        mlp_ratio=4,\n",
      "        num_heads=[\n",
      "            6,\n",
      "            12,\n",
      "            24,\n",
      "            48,\n",
      "        ],\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        patch_norm=True,\n",
      "        pretrain_img_size=384,\n",
      "        qk_scale=None,\n",
      "        qkv_bias=True,\n",
      "        type='SwinTransformer',\n",
      "        window_size=12,\n",
      "        with_cp=False),\n",
      "    bbox_head=[\n",
      "        dict(\n",
      "            anchor_generator=dict(\n",
      "                octave_base_scale=8,\n",
      "                ratios=[\n",
      "                    1.0,\n",
      "                ],\n",
      "                scales_per_octave=1,\n",
      "                strides=[\n",
      "                    4,\n",
      "                    8,\n",
      "                    16,\n",
      "                    32,\n",
      "                    64,\n",
      "                    128,\n",
      "                ],\n",
      "                type='AnchorGenerator'),\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            feat_channels=256,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=24.0, type='GIoULoss'),\n",
      "            loss_centerness=dict(\n",
      "                loss_weight=12.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "            loss_cls=dict(\n",
      "                alpha=0.25,\n",
      "                gamma=2.0,\n",
      "                loss_weight=12.0,\n",
      "                type='FocalLoss',\n",
      "                use_sigmoid=True),\n",
      "            num_classes=10,\n",
      "            stacked_convs=1,\n",
      "            type='CoATSSHead'),\n",
      "    ],\n",
      "    data_preprocessor=dict(\n",
      "        batch_augments=None,\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_mask=False,\n",
      "        pad_seg=False,\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    eval_module='detr',\n",
      "    neck=dict(\n",
      "        act_cfg=None,\n",
      "        in_channels=[\n",
      "            192,\n",
      "            384,\n",
      "            768,\n",
      "            1536,\n",
      "        ],\n",
      "        kernel_size=1,\n",
      "        norm_cfg=dict(num_groups=32, type='GN'),\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='ChannelMapper'),\n",
      "    query_head=dict(\n",
      "        as_two_stage=True,\n",
      "        dn_cfg=dict(\n",
      "            box_noise_scale=1.0,\n",
      "            group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),\n",
      "            label_noise_scale=0.5),\n",
      "        in_channels=2048,\n",
      "        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            beta=2.0,\n",
      "            loss_weight=1.0,\n",
      "            type='QualityFocalLoss',\n",
      "            use_sigmoid=True),\n",
      "        loss_iou=dict(loss_weight=2.0, type='GIoULoss'),\n",
      "        num_classes=10,\n",
      "        num_query=900,\n",
      "        positional_encoding=dict(\n",
      "            normalize=True,\n",
      "            num_feats=128,\n",
      "            temperature=20,\n",
      "            type='SinePositionalEncoding'),\n",
      "        transformer=dict(\n",
      "            decoder=dict(\n",
      "                num_layers=6,\n",
      "                return_intermediate=True,\n",
      "                transformerlayers=dict(\n",
      "                    attn_cfgs=[\n",
      "                        dict(\n",
      "                            dropout=0.0,\n",
      "                            embed_dims=256,\n",
      "                            num_heads=8,\n",
      "                            type='MultiheadAttention'),\n",
      "                        dict(\n",
      "                            dropout=0.0,\n",
      "                            embed_dims=256,\n",
      "                            num_levels=5,\n",
      "                            type='MultiScaleDeformableAttention'),\n",
      "                    ],\n",
      "                    feedforward_channels=2048,\n",
      "                    ffn_dropout=0.0,\n",
      "                    operation_order=(\n",
      "                        'self_attn',\n",
      "                        'norm',\n",
      "                        'cross_attn',\n",
      "                        'norm',\n",
      "                        'ffn',\n",
      "                        'norm',\n",
      "                    ),\n",
      "                    type='DetrTransformerDecoderLayer'),\n",
      "                type='DinoTransformerDecoder'),\n",
      "            encoder=dict(\n",
      "                num_layers=6,\n",
      "                transformerlayers=dict(\n",
      "                    attn_cfgs=dict(\n",
      "                        dropout=0.0,\n",
      "                        embed_dims=256,\n",
      "                        num_levels=5,\n",
      "                        type='MultiScaleDeformableAttention'),\n",
      "                    feedforward_channels=2048,\n",
      "                    ffn_dropout=0.0,\n",
      "                    operation_order=(\n",
      "                        'self_attn',\n",
      "                        'norm',\n",
      "                        'ffn',\n",
      "                        'norm',\n",
      "                    ),\n",
      "                    type='BaseTransformerLayer'),\n",
      "                type='DetrTransformerEncoder',\n",
      "                with_cp=6),\n",
      "            num_co_heads=2,\n",
      "            num_feature_levels=5,\n",
      "            type='CoDinoTransformer',\n",
      "            with_coord_feat=False),\n",
      "        type='CoDINOHead'),\n",
      "    roi_head=[\n",
      "        dict(\n",
      "            bbox_head=dict(\n",
      "                bbox_coder=dict(\n",
      "                    target_means=[\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                        0.0,\n",
      "                    ],\n",
      "                    target_stds=[\n",
      "                        0.1,\n",
      "                        0.1,\n",
      "                        0.2,\n",
      "                        0.2,\n",
      "                    ],\n",
      "                    type='DeltaXYWHBBoxCoder'),\n",
      "                fc_out_channels=1024,\n",
      "                in_channels=256,\n",
      "                loss_bbox=dict(loss_weight=120.0, type='GIoULoss'),\n",
      "                loss_cls=dict(\n",
      "                    loss_weight=12.0,\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False),\n",
      "                num_classes=10,\n",
      "                reg_class_agnostic=False,\n",
      "                reg_decoded_bbox=True,\n",
      "                roi_feat_size=7,\n",
      "                type='Shared2FCBBoxHead'),\n",
      "            bbox_roi_extractor=dict(\n",
      "                featmap_strides=[\n",
      "                    4,\n",
      "                    8,\n",
      "                    16,\n",
      "                    32,\n",
      "                    64,\n",
      "                ],\n",
      "                finest_scale=56,\n",
      "                out_channels=256,\n",
      "                roi_layer=dict(\n",
      "                    output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "                type='SingleRoIExtractor'),\n",
      "            type='CoStandardRoIHead'),\n",
      "    ],\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            octave_base_scale=4,\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales_per_octave=3,\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "                128,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=12.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=12.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=[\n",
      "        dict(max_per_img=300, nms=dict(iou_threshold=0.8, type='soft_nms')),\n",
      "        dict(\n",
      "            rcnn=dict(\n",
      "                max_per_img=100,\n",
      "                nms=dict(iou_threshold=0.5, type='nms'),\n",
      "                score_thr=0.0),\n",
      "            rpn=dict(\n",
      "                max_per_img=1000,\n",
      "                min_bbox_size=0,\n",
      "                nms=dict(iou_threshold=0.7, type='nms'),\n",
      "                nms_pre=1000)),\n",
      "        dict(\n",
      "            max_per_img=100,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.6, type='nms'),\n",
      "            nms_pre=1000,\n",
      "            score_thr=0.0),\n",
      "    ],\n",
      "    train_cfg=[\n",
      "        dict(\n",
      "            assigner=dict(\n",
      "                match_costs=[\n",
      "                    dict(type='FocalLossCost', weight=2.0),\n",
      "                    dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),\n",
      "                    dict(iou_mode='giou', type='IoUCost', weight=2.0),\n",
      "                ],\n",
      "                type='HungarianAssigner')),\n",
      "        dict(\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=False,\n",
      "                    min_pos_iou=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    pos_iou_thr=0.5,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=True,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    type='RandomSampler')),\n",
      "            rpn=dict(\n",
      "                allowed_border=-1,\n",
      "                assigner=dict(\n",
      "                    ignore_iof_thr=-1,\n",
      "                    match_low_quality=True,\n",
      "                    min_pos_iou=0.3,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    pos_iou_thr=0.7,\n",
      "                    type='MaxIoUAssigner'),\n",
      "                debug=False,\n",
      "                pos_weight=-1,\n",
      "                sampler=dict(\n",
      "                    add_gt_as_proposals=False,\n",
      "                    neg_pos_ub=-1,\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    type='RandomSampler')),\n",
      "            rpn_proposal=dict(\n",
      "                max_per_img=1000,\n",
      "                min_bbox_size=0,\n",
      "                nms=dict(iou_threshold=0.7, type='nms'),\n",
      "                nms_pre=4000)),\n",
      "        dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(topk=9, type='ATSSAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1),\n",
      "    ],\n",
      "    type='CoDETR',\n",
      "    use_lsj=True)\n",
      "num_classes = 10\n",
      "num_dec_layer = 6\n",
      "optim_wrapper = dict(\n",
      "    accumulative_counts=1,\n",
      "    optimizer=dict(lr=1e-05, type='AdamW', weight_decay=0.0001),\n",
      "    type='AmpOptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=50,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'\n",
      "resume = True\n",
      "resume_optimizer = False\n",
      "test_cfg = dict(_scope_='mmdet', type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        _scope_='mmdet',\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Resize'),\n",
      "            dict(\n",
      "                pad_val=dict(img=(\n",
      "                    114,\n",
      "                    114,\n",
      "                    114,\n",
      "                )),\n",
      "                size=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                type='Pad'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(_scope_='mmdet', shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    _scope_='mmdet',\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='Resize'),\n",
      "    dict(pad_val=dict(img=(\n",
      "        114,\n",
      "        114,\n",
      "        114,\n",
      "    )), size=(\n",
      "        1280,\n",
      "        1280,\n",
      "    ), type='Pad'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=50, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        dataset=dict(\n",
      "            ann_file='/data/ephemeral/home/dataset/train_split.json',\n",
      "            data_prefix=dict(img='/data/ephemeral/home/dataset/'),\n",
      "            data_root='/data/ephemeral/home/dataset/',\n",
      "            filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "            metainfo=dict(\n",
      "                classes=(\n",
      "                    'General trash',\n",
      "                    'Paper',\n",
      "                    'Paper pack',\n",
      "                    'Metal',\n",
      "                    'Glass',\n",
      "                    'Plastic',\n",
      "                    'Styrofoam',\n",
      "                    'Plastic bag',\n",
      "                    'Battery',\n",
      "                    'Clothing',\n",
      "                )),\n",
      "            pipeline=[\n",
      "                dict(backend_args=None, type='LoadImageFromFile'),\n",
      "                dict(\n",
      "                    type='LoadAnnotations',\n",
      "                    with_bbox=True,\n",
      "                    with_label=True,\n",
      "                    with_mask=False),\n",
      "                dict(\n",
      "                    keep_ratio=True,\n",
      "                    ratio_range=(\n",
      "                        0.1,\n",
      "                        2.0,\n",
      "                    ),\n",
      "                    scale=(\n",
      "                        1280,\n",
      "                        1280,\n",
      "                    ),\n",
      "                    type='RandomResize'),\n",
      "                dict(\n",
      "                    allow_negative_crop=True,\n",
      "                    crop_size=(\n",
      "                        1280,\n",
      "                        1280,\n",
      "                    ),\n",
      "                    crop_type='absolute_range',\n",
      "                    recompute_bbox=True,\n",
      "                    type='RandomCrop'),\n",
      "                dict(min_gt_bbox_wh=(\n",
      "                    0.01,\n",
      "                    0.01,\n",
      "                ), type='FilterAnnotations'),\n",
      "                dict(prob=0.5, type='RandomFlip'),\n",
      "                dict(size=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ), type='Pad'),\n",
      "            ],\n",
      "            type='CocoDataset'),\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                bbox_clip_border=True,\n",
      "                flip_ratio=0.5,\n",
      "                img_scale=(\n",
      "                    1280,\n",
      "                    1280,\n",
      "                ),\n",
      "                max_iters=15,\n",
      "                pad_val=114.0,\n",
      "                ratio_range=(\n",
      "                    0.1,\n",
      "                    2.0,\n",
      "                ),\n",
      "                type='MixUp'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='MultiImageMixDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=False,\n",
      "    prefetch_factor=2,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(max_num_pasted=100, type='CopyPaste'),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(_scope_='mmdet', type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='/data/ephemeral/home/dataset/val_split.json',\n",
      "        data_prefix=dict(img='/data/ephemeral/home/dataset/'),\n",
      "        data_root='/data/ephemeral/home/dataset/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'General trash',\n",
      "                'Paper',\n",
      "                'Paper pack',\n",
      "                'Metal',\n",
      "                'Glass',\n",
      "                'Plastic',\n",
      "                'Styrofoam',\n",
      "                'Plastic bag',\n",
      "                'Battery',\n",
      "                'Clothing',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='LoadAnnotations',\n",
      "                with_bbox=True,\n",
      "                with_label=True,\n",
      "                with_mask=False),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Resize'),\n",
      "            dict(size=(\n",
      "                1280,\n",
      "                1280,\n",
      "            ), type='Pad'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=False,\n",
      "    prefetch_factor=2,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    _scope_='mmdet',\n",
      "    ann_file='/data/ephemeral/home/dataset/val_split.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(_scope_='mmdet', type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='Visualizer',\n",
      "    vis_backends=[\n",
      "        dict(\n",
      "            commit=True,\n",
      "            define_metric_cfg=None,\n",
      "            init_kwargs=dict(\n",
      "                allow_val_change=True,\n",
      "                name='co_dino_5scale_swin_l_lsj_20241024_152536',\n",
      "                project='Object_detection'),\n",
      "            log_code_name=None,\n",
      "            save_dir='./work_dirs/co_dino_custom',\n",
      "            type='WandbVisBackend',\n",
      "            watch_kwargs=None),\n",
      "    ])\n",
      "work_dir = './work_dirs/co_dino_custom'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wxkitv4e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">co_dino_5scale_swin_l_lsj_20241024_152536</strong> at: <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/wxkitv4e' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/wxkitv4e</a><br/> View project at: <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241024_152541-wxkitv4e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wxkitv4e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./work_dirs/co_dino_custom/wandb/run-20241024_152544-dosmoxbx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/dosmoxbx' target=\"_blank\">co_dino_5scale_swin_l_lsj_20241024_152536</a></strong> to <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/youngtae0818-naver/Object_detection' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/youngtae0818-naver/Object_detection/runs/dosmoxbx' target=\"_blank\">https://wandb.ai/youngtae0818-naver/Object_detection/runs/dosmoxbx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/baseline/mmdetection/../mmdetection/projects/CO-DETR/codetr/transformer.py:1325: UserWarning: If you want to reduce GPU memory usage,                               please install fairscale by executing the                               following command: pip install fairscale.\n",
      "  warnings.warn('If you want to reduce GPU memory usage, \\\n",
      "/data/ephemeral/home/baseline/mmdetection/mmdet/models/dense_heads/anchor_head.py:108: UserWarning: DeprecationWarning: `num_anchors` is deprecated, for consistency or also use `num_base_priors` instead\n",
      "  warnings.warn('DeprecationWarning: `num_anchors` is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/24 15:26:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "10/24 15:26:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      "(LOWEST      ) EarlyStoppingHook                  \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "# # 사전 학습된 모델 경로 설정\n",
    "# pretrained_model_path = '/data/ephemeral/home/baseline/mmdetection/work_dirs/co_dino_custom/co_dino_5scale_lsj_swin_large_1x_coco-3af73af2.pth'\n",
    "\n",
    "# # cfg 설정에 사전 학습된 모델 불러오기 추가\n",
    "# cfg.load_from = pretrained_model_path  # 사전 학습된 모델 경로 지정\n",
    "# cfg.resume = False  # 학습을 처음부터 시작\n",
    "\n",
    "# 설정 파일을 통해 러너 생성\n",
    "runner = Runner.from_cfg(cfg)\n",
    "\n",
    "# ---------------------- 레이어 동결 및 Unfreeze 설정 시작 ----------------------\n",
    "\n",
    "# 1. Backbone의 frozen_stages는 이미 설정되어 있으므로 첫 번째 stage는 유지\n",
    "\n",
    "# 2. 모든 레이어를 동결\n",
    "for name, param in runner.model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. 필요한 레이어 Unfreeze\n",
    "# bbox_head Unfreeze\n",
    "for param in runner.model.bbox_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# backbone의 마지막 두 레이어 Unfreeze\n",
    "for name, param in runner.model.backbone.named_parameters():\n",
    "    if 'stages.2' in name or 'stages.3' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# query_head Unfreeze\n",
    "for name, param in runner.model.query_head.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# roi_head Unfreeze\n",
    "for name, param in runner.model.roi_head.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# neck Unfreeze\n",
    "for name, param in runner.model.neck.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# rpn_head Unfreeze\n",
    "for name, param in runner.model.rpn_head.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 7. Optimizer 설정 수정\n",
    "cfg.optim_wrapper.paramwise_cfg = dict(\n",
    "    custom_keys={\n",
    "        'backbone.stages.2': dict(lr_mult=0.1),\n",
    "        'backbone.stages.3': dict(lr_mult=0.1),\n",
    "        'neck': dict(lr_mult=1.0),\n",
    "        'query_head': dict(lr_mult=1.0),\n",
    "        'bbox_head': dict(lr_mult=1.0),\n",
    "        'roi_head': dict(lr_mult=1.0),\n",
    "        'rpn_head': dict(lr_mult=1.0)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.projection.weight\n",
      "patch_embed.projection.bias\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.bias\n",
      "stages.0.blocks.0.norm1.weight\n",
      "stages.0.blocks.0.norm1.bias\n",
      "stages.0.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.0.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.0.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.0.blocks.0.attn.w_msa.proj.weight\n",
      "stages.0.blocks.0.attn.w_msa.proj.bias\n",
      "stages.0.blocks.0.norm2.weight\n",
      "stages.0.blocks.0.norm2.bias\n",
      "stages.0.blocks.0.ffn.layers.0.0.weight\n",
      "stages.0.blocks.0.ffn.layers.0.0.bias\n",
      "stages.0.blocks.0.ffn.layers.1.weight\n",
      "stages.0.blocks.0.ffn.layers.1.bias\n",
      "stages.0.blocks.1.norm1.weight\n",
      "stages.0.blocks.1.norm1.bias\n",
      "stages.0.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.0.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.0.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.0.blocks.1.attn.w_msa.proj.weight\n",
      "stages.0.blocks.1.attn.w_msa.proj.bias\n",
      "stages.0.blocks.1.norm2.weight\n",
      "stages.0.blocks.1.norm2.bias\n",
      "stages.0.blocks.1.ffn.layers.0.0.weight\n",
      "stages.0.blocks.1.ffn.layers.0.0.bias\n",
      "stages.0.blocks.1.ffn.layers.1.weight\n",
      "stages.0.blocks.1.ffn.layers.1.bias\n",
      "stages.0.downsample.norm.weight\n",
      "stages.0.downsample.norm.bias\n",
      "stages.0.downsample.reduction.weight\n",
      "stages.1.blocks.0.norm1.weight\n",
      "stages.1.blocks.0.norm1.bias\n",
      "stages.1.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.1.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.1.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.1.blocks.0.attn.w_msa.proj.weight\n",
      "stages.1.blocks.0.attn.w_msa.proj.bias\n",
      "stages.1.blocks.0.norm2.weight\n",
      "stages.1.blocks.0.norm2.bias\n",
      "stages.1.blocks.0.ffn.layers.0.0.weight\n",
      "stages.1.blocks.0.ffn.layers.0.0.bias\n",
      "stages.1.blocks.0.ffn.layers.1.weight\n",
      "stages.1.blocks.0.ffn.layers.1.bias\n",
      "stages.1.blocks.1.norm1.weight\n",
      "stages.1.blocks.1.norm1.bias\n",
      "stages.1.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.1.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.1.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.1.blocks.1.attn.w_msa.proj.weight\n",
      "stages.1.blocks.1.attn.w_msa.proj.bias\n",
      "stages.1.blocks.1.norm2.weight\n",
      "stages.1.blocks.1.norm2.bias\n",
      "stages.1.blocks.1.ffn.layers.0.0.weight\n",
      "stages.1.blocks.1.ffn.layers.0.0.bias\n",
      "stages.1.blocks.1.ffn.layers.1.weight\n",
      "stages.1.blocks.1.ffn.layers.1.bias\n",
      "stages.1.downsample.norm.weight\n",
      "stages.1.downsample.norm.bias\n",
      "stages.1.downsample.reduction.weight\n",
      "stages.2.blocks.0.norm1.weight\n",
      "stages.2.blocks.0.norm1.bias\n",
      "stages.2.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.0.attn.w_msa.proj.weight\n",
      "stages.2.blocks.0.attn.w_msa.proj.bias\n",
      "stages.2.blocks.0.norm2.weight\n",
      "stages.2.blocks.0.norm2.bias\n",
      "stages.2.blocks.0.ffn.layers.0.0.weight\n",
      "stages.2.blocks.0.ffn.layers.0.0.bias\n",
      "stages.2.blocks.0.ffn.layers.1.weight\n",
      "stages.2.blocks.0.ffn.layers.1.bias\n",
      "stages.2.blocks.1.norm1.weight\n",
      "stages.2.blocks.1.norm1.bias\n",
      "stages.2.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.1.attn.w_msa.proj.weight\n",
      "stages.2.blocks.1.attn.w_msa.proj.bias\n",
      "stages.2.blocks.1.norm2.weight\n",
      "stages.2.blocks.1.norm2.bias\n",
      "stages.2.blocks.1.ffn.layers.0.0.weight\n",
      "stages.2.blocks.1.ffn.layers.0.0.bias\n",
      "stages.2.blocks.1.ffn.layers.1.weight\n",
      "stages.2.blocks.1.ffn.layers.1.bias\n",
      "stages.2.blocks.2.norm1.weight\n",
      "stages.2.blocks.2.norm1.bias\n",
      "stages.2.blocks.2.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.2.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.2.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.2.attn.w_msa.proj.weight\n",
      "stages.2.blocks.2.attn.w_msa.proj.bias\n",
      "stages.2.blocks.2.norm2.weight\n",
      "stages.2.blocks.2.norm2.bias\n",
      "stages.2.blocks.2.ffn.layers.0.0.weight\n",
      "stages.2.blocks.2.ffn.layers.0.0.bias\n",
      "stages.2.blocks.2.ffn.layers.1.weight\n",
      "stages.2.blocks.2.ffn.layers.1.bias\n",
      "stages.2.blocks.3.norm1.weight\n",
      "stages.2.blocks.3.norm1.bias\n",
      "stages.2.blocks.3.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.3.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.3.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.3.attn.w_msa.proj.weight\n",
      "stages.2.blocks.3.attn.w_msa.proj.bias\n",
      "stages.2.blocks.3.norm2.weight\n",
      "stages.2.blocks.3.norm2.bias\n",
      "stages.2.blocks.3.ffn.layers.0.0.weight\n",
      "stages.2.blocks.3.ffn.layers.0.0.bias\n",
      "stages.2.blocks.3.ffn.layers.1.weight\n",
      "stages.2.blocks.3.ffn.layers.1.bias\n",
      "stages.2.blocks.4.norm1.weight\n",
      "stages.2.blocks.4.norm1.bias\n",
      "stages.2.blocks.4.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.4.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.4.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.4.attn.w_msa.proj.weight\n",
      "stages.2.blocks.4.attn.w_msa.proj.bias\n",
      "stages.2.blocks.4.norm2.weight\n",
      "stages.2.blocks.4.norm2.bias\n",
      "stages.2.blocks.4.ffn.layers.0.0.weight\n",
      "stages.2.blocks.4.ffn.layers.0.0.bias\n",
      "stages.2.blocks.4.ffn.layers.1.weight\n",
      "stages.2.blocks.4.ffn.layers.1.bias\n",
      "stages.2.blocks.5.norm1.weight\n",
      "stages.2.blocks.5.norm1.bias\n",
      "stages.2.blocks.5.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.5.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.5.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.5.attn.w_msa.proj.weight\n",
      "stages.2.blocks.5.attn.w_msa.proj.bias\n",
      "stages.2.blocks.5.norm2.weight\n",
      "stages.2.blocks.5.norm2.bias\n",
      "stages.2.blocks.5.ffn.layers.0.0.weight\n",
      "stages.2.blocks.5.ffn.layers.0.0.bias\n",
      "stages.2.blocks.5.ffn.layers.1.weight\n",
      "stages.2.blocks.5.ffn.layers.1.bias\n",
      "stages.2.blocks.6.norm1.weight\n",
      "stages.2.blocks.6.norm1.bias\n",
      "stages.2.blocks.6.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.6.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.6.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.6.attn.w_msa.proj.weight\n",
      "stages.2.blocks.6.attn.w_msa.proj.bias\n",
      "stages.2.blocks.6.norm2.weight\n",
      "stages.2.blocks.6.norm2.bias\n",
      "stages.2.blocks.6.ffn.layers.0.0.weight\n",
      "stages.2.blocks.6.ffn.layers.0.0.bias\n",
      "stages.2.blocks.6.ffn.layers.1.weight\n",
      "stages.2.blocks.6.ffn.layers.1.bias\n",
      "stages.2.blocks.7.norm1.weight\n",
      "stages.2.blocks.7.norm1.bias\n",
      "stages.2.blocks.7.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.7.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.7.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.7.attn.w_msa.proj.weight\n",
      "stages.2.blocks.7.attn.w_msa.proj.bias\n",
      "stages.2.blocks.7.norm2.weight\n",
      "stages.2.blocks.7.norm2.bias\n",
      "stages.2.blocks.7.ffn.layers.0.0.weight\n",
      "stages.2.blocks.7.ffn.layers.0.0.bias\n",
      "stages.2.blocks.7.ffn.layers.1.weight\n",
      "stages.2.blocks.7.ffn.layers.1.bias\n",
      "stages.2.blocks.8.norm1.weight\n",
      "stages.2.blocks.8.norm1.bias\n",
      "stages.2.blocks.8.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.8.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.8.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.8.attn.w_msa.proj.weight\n",
      "stages.2.blocks.8.attn.w_msa.proj.bias\n",
      "stages.2.blocks.8.norm2.weight\n",
      "stages.2.blocks.8.norm2.bias\n",
      "stages.2.blocks.8.ffn.layers.0.0.weight\n",
      "stages.2.blocks.8.ffn.layers.0.0.bias\n",
      "stages.2.blocks.8.ffn.layers.1.weight\n",
      "stages.2.blocks.8.ffn.layers.1.bias\n",
      "stages.2.blocks.9.norm1.weight\n",
      "stages.2.blocks.9.norm1.bias\n",
      "stages.2.blocks.9.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.9.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.9.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.9.attn.w_msa.proj.weight\n",
      "stages.2.blocks.9.attn.w_msa.proj.bias\n",
      "stages.2.blocks.9.norm2.weight\n",
      "stages.2.blocks.9.norm2.bias\n",
      "stages.2.blocks.9.ffn.layers.0.0.weight\n",
      "stages.2.blocks.9.ffn.layers.0.0.bias\n",
      "stages.2.blocks.9.ffn.layers.1.weight\n",
      "stages.2.blocks.9.ffn.layers.1.bias\n",
      "stages.2.blocks.10.norm1.weight\n",
      "stages.2.blocks.10.norm1.bias\n",
      "stages.2.blocks.10.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.10.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.10.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.10.attn.w_msa.proj.weight\n",
      "stages.2.blocks.10.attn.w_msa.proj.bias\n",
      "stages.2.blocks.10.norm2.weight\n",
      "stages.2.blocks.10.norm2.bias\n",
      "stages.2.blocks.10.ffn.layers.0.0.weight\n",
      "stages.2.blocks.10.ffn.layers.0.0.bias\n",
      "stages.2.blocks.10.ffn.layers.1.weight\n",
      "stages.2.blocks.10.ffn.layers.1.bias\n",
      "stages.2.blocks.11.norm1.weight\n",
      "stages.2.blocks.11.norm1.bias\n",
      "stages.2.blocks.11.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.11.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.11.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.11.attn.w_msa.proj.weight\n",
      "stages.2.blocks.11.attn.w_msa.proj.bias\n",
      "stages.2.blocks.11.norm2.weight\n",
      "stages.2.blocks.11.norm2.bias\n",
      "stages.2.blocks.11.ffn.layers.0.0.weight\n",
      "stages.2.blocks.11.ffn.layers.0.0.bias\n",
      "stages.2.blocks.11.ffn.layers.1.weight\n",
      "stages.2.blocks.11.ffn.layers.1.bias\n",
      "stages.2.blocks.12.norm1.weight\n",
      "stages.2.blocks.12.norm1.bias\n",
      "stages.2.blocks.12.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.12.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.12.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.12.attn.w_msa.proj.weight\n",
      "stages.2.blocks.12.attn.w_msa.proj.bias\n",
      "stages.2.blocks.12.norm2.weight\n",
      "stages.2.blocks.12.norm2.bias\n",
      "stages.2.blocks.12.ffn.layers.0.0.weight\n",
      "stages.2.blocks.12.ffn.layers.0.0.bias\n",
      "stages.2.blocks.12.ffn.layers.1.weight\n",
      "stages.2.blocks.12.ffn.layers.1.bias\n",
      "stages.2.blocks.13.norm1.weight\n",
      "stages.2.blocks.13.norm1.bias\n",
      "stages.2.blocks.13.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.13.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.13.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.13.attn.w_msa.proj.weight\n",
      "stages.2.blocks.13.attn.w_msa.proj.bias\n",
      "stages.2.blocks.13.norm2.weight\n",
      "stages.2.blocks.13.norm2.bias\n",
      "stages.2.blocks.13.ffn.layers.0.0.weight\n",
      "stages.2.blocks.13.ffn.layers.0.0.bias\n",
      "stages.2.blocks.13.ffn.layers.1.weight\n",
      "stages.2.blocks.13.ffn.layers.1.bias\n",
      "stages.2.blocks.14.norm1.weight\n",
      "stages.2.blocks.14.norm1.bias\n",
      "stages.2.blocks.14.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.14.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.14.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.14.attn.w_msa.proj.weight\n",
      "stages.2.blocks.14.attn.w_msa.proj.bias\n",
      "stages.2.blocks.14.norm2.weight\n",
      "stages.2.blocks.14.norm2.bias\n",
      "stages.2.blocks.14.ffn.layers.0.0.weight\n",
      "stages.2.blocks.14.ffn.layers.0.0.bias\n",
      "stages.2.blocks.14.ffn.layers.1.weight\n",
      "stages.2.blocks.14.ffn.layers.1.bias\n",
      "stages.2.blocks.15.norm1.weight\n",
      "stages.2.blocks.15.norm1.bias\n",
      "stages.2.blocks.15.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.15.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.15.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.15.attn.w_msa.proj.weight\n",
      "stages.2.blocks.15.attn.w_msa.proj.bias\n",
      "stages.2.blocks.15.norm2.weight\n",
      "stages.2.blocks.15.norm2.bias\n",
      "stages.2.blocks.15.ffn.layers.0.0.weight\n",
      "stages.2.blocks.15.ffn.layers.0.0.bias\n",
      "stages.2.blocks.15.ffn.layers.1.weight\n",
      "stages.2.blocks.15.ffn.layers.1.bias\n",
      "stages.2.blocks.16.norm1.weight\n",
      "stages.2.blocks.16.norm1.bias\n",
      "stages.2.blocks.16.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.16.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.16.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.16.attn.w_msa.proj.weight\n",
      "stages.2.blocks.16.attn.w_msa.proj.bias\n",
      "stages.2.blocks.16.norm2.weight\n",
      "stages.2.blocks.16.norm2.bias\n",
      "stages.2.blocks.16.ffn.layers.0.0.weight\n",
      "stages.2.blocks.16.ffn.layers.0.0.bias\n",
      "stages.2.blocks.16.ffn.layers.1.weight\n",
      "stages.2.blocks.16.ffn.layers.1.bias\n",
      "stages.2.blocks.17.norm1.weight\n",
      "stages.2.blocks.17.norm1.bias\n",
      "stages.2.blocks.17.attn.w_msa.relative_position_bias_table\n",
      "stages.2.blocks.17.attn.w_msa.qkv.weight\n",
      "stages.2.blocks.17.attn.w_msa.qkv.bias\n",
      "stages.2.blocks.17.attn.w_msa.proj.weight\n",
      "stages.2.blocks.17.attn.w_msa.proj.bias\n",
      "stages.2.blocks.17.norm2.weight\n",
      "stages.2.blocks.17.norm2.bias\n",
      "stages.2.blocks.17.ffn.layers.0.0.weight\n",
      "stages.2.blocks.17.ffn.layers.0.0.bias\n",
      "stages.2.blocks.17.ffn.layers.1.weight\n",
      "stages.2.blocks.17.ffn.layers.1.bias\n",
      "stages.2.downsample.norm.weight\n",
      "stages.2.downsample.norm.bias\n",
      "stages.2.downsample.reduction.weight\n",
      "stages.3.blocks.0.norm1.weight\n",
      "stages.3.blocks.0.norm1.bias\n",
      "stages.3.blocks.0.attn.w_msa.relative_position_bias_table\n",
      "stages.3.blocks.0.attn.w_msa.qkv.weight\n",
      "stages.3.blocks.0.attn.w_msa.qkv.bias\n",
      "stages.3.blocks.0.attn.w_msa.proj.weight\n",
      "stages.3.blocks.0.attn.w_msa.proj.bias\n",
      "stages.3.blocks.0.norm2.weight\n",
      "stages.3.blocks.0.norm2.bias\n",
      "stages.3.blocks.0.ffn.layers.0.0.weight\n",
      "stages.3.blocks.0.ffn.layers.0.0.bias\n",
      "stages.3.blocks.0.ffn.layers.1.weight\n",
      "stages.3.blocks.0.ffn.layers.1.bias\n",
      "stages.3.blocks.1.norm1.weight\n",
      "stages.3.blocks.1.norm1.bias\n",
      "stages.3.blocks.1.attn.w_msa.relative_position_bias_table\n",
      "stages.3.blocks.1.attn.w_msa.qkv.weight\n",
      "stages.3.blocks.1.attn.w_msa.qkv.bias\n",
      "stages.3.blocks.1.attn.w_msa.proj.weight\n",
      "stages.3.blocks.1.attn.w_msa.proj.bias\n",
      "stages.3.blocks.1.norm2.weight\n",
      "stages.3.blocks.1.norm2.bias\n",
      "stages.3.blocks.1.ffn.layers.0.0.weight\n",
      "stages.3.blocks.1.ffn.layers.0.0.bias\n",
      "stages.3.blocks.1.ffn.layers.1.weight\n",
      "stages.3.blocks.1.ffn.layers.1.bias\n",
      "norm0.weight\n",
      "norm0.bias\n",
      "norm1.weight\n",
      "norm1.bias\n",
      "norm2.weight\n",
      "norm2.bias\n",
      "norm3.weight\n",
      "norm3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in runner.model.backbone.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "10/24 15:26:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\n",
      "Loads checkpoint by local backend from path: ./work_dirs/co_dino_custom/epoch_9.pth\n",
      "10/24 15:26:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from ./work_dirs/co_dino_custom/epoch_9.pth\n",
      "10/24 15:26:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - resumed epoch: 9, iter: 39420\n",
      "10/24 15:26:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/24 15:26:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "10/24 15:26:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /data/ephemeral/home/baseline/mmdetection/work_dirs/co_dino_custom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/baseline/mmdetection/mmdet/models/layers/positional_encoding.py:103: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature**(2 * (dim_t // 2) / self.num_feats)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/ephemeral/home/baseline/mmdetection/../mmdetection/projects/CO-DETR/codetr/transformer.py:991: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = 10000**(2 * (dim_t // 2) / pos_feat)\n",
      "/opt/conda/lib/python3.10/site-packages/mmcv/cnn/bricks/transformer.py:819: UserWarning: Use same attn_mask in all attentions in DetrTransformerDecoderLayer \n",
      "  warnings.warn(f'Use same attn_mask in all attentions in '\n",
      "/data/ephemeral/home/baseline/mmdetection/mmdet/models/task_modules/samplers/sampling_result.py:126: UserWarning: DeprecationWarning: bboxes is deprecated, please use \"priors\" instead\n",
      "  warnings.warn('DeprecationWarning: bboxes is deprecated, '\n",
      "/data/ephemeral/home/baseline/mmdetection/../mmdetection/projects/CO-DETR/codetr/transformer.py:695: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/24 15:27:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][  50/4380]  lr: 1.0000e-04  eta: 3 days, 15:07:05  time: 1.7469  data_time: 0.0194  memory: 23790  loss: 40.0350  loss_cls: 0.4855  loss_bbox: 0.1591  loss_iou: 0.2376  d0.loss_cls: 0.6312  d0.loss_bbox: 0.1796  d0.loss_iou: 0.2624  d1.loss_cls: 0.5405  d1.loss_bbox: 0.1594  d1.loss_iou: 0.2403  d2.loss_cls: 0.5128  d2.loss_bbox: 0.1527  d2.loss_iou: 0.2324  d3.loss_cls: 0.4998  d3.loss_bbox: 0.1567  d3.loss_iou: 0.2335  d4.loss_cls: 0.4893  d4.loss_bbox: 0.1568  d4.loss_iou: 0.2348  enc_loss_cls: 0.5809  enc_loss_bbox: 0.2117  enc_loss_iou: 0.2998  dn_loss_cls: 0.0892  dn_loss_bbox: 0.3515  dn_loss_iou: 0.3391  d0.dn_loss_cls: 0.1440  d0.dn_loss_bbox: 0.5166  d0.dn_loss_iou: 0.5008  d1.dn_loss_cls: 0.0972  d1.dn_loss_bbox: 0.3832  d1.dn_loss_iou: 0.3755  d2.dn_loss_cls: 0.0900  d2.dn_loss_bbox: 0.3552  d2.dn_loss_iou: 0.3471  d3.dn_loss_cls: 0.0901  d3.dn_loss_bbox: 0.3513  d3.dn_loss_iou: 0.3388  d4.dn_loss_cls: 0.0891  d4.dn_loss_bbox: 0.3514  d4.dn_loss_iou: 0.3390  loss_rpn_cls: 0.1327  loss_rpn_bbox: 0.2101  loss_cls0: 3.6317  acc0: 77.5391  loss_bbox0: 3.0065  loss_cls1: 3.3810  loss_bbox1: 3.8084  loss_centerness1: 7.2052  loss_cls_aux0: 0.2495  loss_bbox_aux0: 0.0834  loss_iou_aux0: 0.1139  d0.loss_cls_aux0: 0.2299  d0.loss_bbox_aux0: 0.2269  d0.loss_iou_aux0: 0.2549  d1.loss_cls_aux0: 0.2482  d1.loss_bbox_aux0: 0.1144  d1.loss_iou_aux0: 0.1499  d2.loss_cls_aux0: 0.2480  d2.loss_bbox_aux0: 0.0881  d2.loss_iou_aux0: 0.1199  d3.loss_cls_aux0: 0.2508  d3.loss_bbox_aux0: 0.0833  d3.loss_iou_aux0: 0.1132  d4.loss_cls_aux0: 0.2454  d4.loss_bbox_aux0: 0.0833  d4.loss_iou_aux0: 0.1135  loss_cls_aux1: 0.2431  loss_bbox_aux1: 0.1551  loss_iou_aux1: 0.1877  d0.loss_cls_aux1: 0.2220  d0.loss_bbox_aux1: 0.2814  d0.loss_iou_aux1: 0.3309  d1.loss_cls_aux1: 0.2441  d1.loss_bbox_aux1: 0.1833  d1.loss_iou_aux1: 0.2233  d2.loss_cls_aux1: 0.2483  d2.loss_bbox_aux1: 0.1538  d2.loss_iou_aux1: 0.1924  d3.loss_cls_aux1: 0.2459  d3.loss_bbox_aux1: 0.1550  d3.loss_iou_aux1: 0.1873  d4.loss_cls_aux1: 0.2410  d4.loss_bbox_aux1: 0.1551  d4.loss_iou_aux1: 0.1875\n",
      "10/24 15:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][ 100/4380]  lr: 1.0000e-04  eta: 3 days, 13:48:32  time: 1.6954  data_time: 0.0101  memory: 23819  loss: 46.7786  loss_cls: 0.5708  loss_bbox: 0.2071  loss_iou: 0.3115  d0.loss_cls: 0.6336  d0.loss_bbox: 0.2197  d0.loss_iou: 0.3449  d1.loss_cls: 0.5984  d1.loss_bbox: 0.2081  d1.loss_iou: 0.3227  d2.loss_cls: 0.5715  d2.loss_bbox: 0.2048  d2.loss_iou: 0.3102  d3.loss_cls: 0.5497  d3.loss_bbox: 0.2104  d3.loss_iou: 0.3145  d4.loss_cls: 0.5612  d4.loss_bbox: 0.2090  d4.loss_iou: 0.3096  enc_loss_cls: 0.6095  enc_loss_bbox: 0.2608  enc_loss_iou: 0.3896  dn_loss_cls: 0.1001  dn_loss_bbox: 0.4290  dn_loss_iou: 0.4374  d0.dn_loss_cls: 0.1421  d0.dn_loss_bbox: 0.5506  d0.dn_loss_iou: 0.5712  d1.dn_loss_cls: 0.1013  d1.dn_loss_bbox: 0.4563  d1.dn_loss_iou: 0.4741  d2.dn_loss_cls: 0.0979  d2.dn_loss_bbox: 0.4316  d2.dn_loss_iou: 0.4443  d3.dn_loss_cls: 0.0977  d3.dn_loss_bbox: 0.4290  d3.dn_loss_iou: 0.4376  d4.dn_loss_cls: 0.0990  d4.dn_loss_bbox: 0.4290  d4.dn_loss_iou: 0.4375  loss_rpn_cls: 0.1660  loss_rpn_bbox: 0.3013  loss_cls0: 4.5247  acc0: 89.4531  loss_bbox0: 4.0916  loss_cls1: 3.7321  loss_bbox1: 4.7284  loss_centerness1: 7.2122  loss_cls_aux0: 0.2445  loss_bbox_aux0: 0.1366  loss_iou_aux0: 0.1677  d0.loss_cls_aux0: 0.2153  d0.loss_bbox_aux0: 0.2704  d0.loss_iou_aux0: 0.3116  d1.loss_cls_aux0: 0.2285  d1.loss_bbox_aux0: 0.1703  d1.loss_iou_aux0: 0.2073  d2.loss_cls_aux0: 0.2354  d2.loss_bbox_aux0: 0.1431  d2.loss_iou_aux0: 0.1735  d3.loss_cls_aux0: 0.2356  d3.loss_bbox_aux0: 0.1364  d3.loss_iou_aux0: 0.1670  d4.loss_cls_aux0: 0.2412  d4.loss_bbox_aux0: 0.1365  d4.loss_iou_aux0: 0.1673  loss_cls_aux1: 0.2370  loss_bbox_aux1: 0.1831  loss_iou_aux1: 0.2507  d0.loss_cls_aux1: 0.2074  d0.loss_bbox_aux1: 0.3249  d0.loss_iou_aux1: 0.4110  d1.loss_cls_aux1: 0.2190  d1.loss_bbox_aux1: 0.2196  d1.loss_iou_aux1: 0.2950  d2.loss_cls_aux1: 0.2268  d2.loss_bbox_aux1: 0.1874  d2.loss_iou_aux1: 0.2564  d3.loss_cls_aux1: 0.2317  d3.loss_bbox_aux1: 0.1829  d3.loss_iou_aux1: 0.2504  d4.loss_cls_aux1: 0.2337  d4.loss_bbox_aux1: 0.1831  d4.loss_iou_aux1: 0.2505\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation annotation file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_ann_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/runner.py:1777\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_compile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1777\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/loops.py:96\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mval_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_begin\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mmengine/runner/loops.py:111\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_iter(idx, data_batch)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:297\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 297\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:508\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43manswer_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     deliver_challenge(c, authkey)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:752\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[0;32m--> 752\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m message[:\u001b[38;5;28mlen\u001b[39m(CHALLENGE)] \u001b[38;5;241m==\u001b[39m CHALLENGE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message\n\u001b[1;32m    754\u001b[0m message \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;28mlen\u001b[39m(CHALLENGE):]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:421\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxsize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m size \u001b[38;5;241m>\u001b[39m maxsize:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습 시작 전에 검증 어노테이션 파일 존재 여부 확인\n",
    "if not os.path.exists(val_ann_file):\n",
    "    raise FileNotFoundError(f'Validation annotation file not found: {val_ann_file}')\n",
    "\n",
    "# 학습 시작\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pipeline:\n",
      "{'type': 'LoadImageFromFile', 'backend_args': None}\n",
      "{'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': False}\n",
      "{'type': 'RandomResize', 'scale': (1024, 1024), 'ratio_range': (0.8, 1.25), 'keep_ratio': True}\n",
      "{'type': 'RandomCrop', 'crop_type': 'absolute_range', 'crop_size': (1024, 1024), 'recompute_bbox': True, 'allow_negative_crop': True}\n",
      "{'type': 'FilterAnnotations', 'min_gt_bbox_wh': (0.01, 0.01)}\n",
      "{'type': 'RandomFlip', 'prob': 0.5}\n",
      "{'type': 'Pad', 'size': (1024, 1024)}\n",
      "{'type': 'PackDetInputs'}\n",
      "Validation Pipeline:\n",
      "{'type': 'LoadImageFromFile', 'backend_args': None}\n",
      "{'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': False}\n",
      "{'type': 'RandomResize', 'scale': (1024, 1024), 'ratio_range': (0.8, 1.25), 'keep_ratio': True}\n",
      "{'type': 'RandomCrop', 'crop_type': 'absolute_range', 'crop_size': (1024, 1024), 'recompute_bbox': True, 'allow_negative_crop': True}\n",
      "{'type': 'FilterAnnotations', 'min_gt_bbox_wh': (0.01, 0.01)}\n",
      "{'type': 'RandomFlip', 'prob': 0.5}\n",
      "{'type': 'Pad', 'size': (1024, 1024)}\n",
      "{'type': 'PackDetInputs'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Pipeline:\")\n",
    "for step in cfg.train_dataloader.dataset.pipeline:\n",
    "    print(step)\n",
    "\n",
    "print(\"Validation Pipeline:\")\n",
    "for step in cfg.val_dataloader.dataset.pipeline:\n",
    "    print(step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
